{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity Access Management 101\n",
    "\n",
    "IAM - allows you to manage users and their level of access to the AWS console.\n",
    "\n",
    "It is important to understand IAM and how it works, both for the exam and for adiminstrating a company's AWS account in real life.\n",
    "\n",
    "### Key features of IAM\n",
    "\n",
    "Identity Access Management offers the following features:\n",
    "- centralised control of your AWS account\n",
    "- shared access to your AWS account\n",
    "- granular permissions\n",
    "- identity federeation (including active dir like Facebook, google)\n",
    "- multifactor authentication\n",
    "- provide temporary access for users/ devices and services where necessary\n",
    "- allows you to set up your own password rotation policy\n",
    "- integrates with many different AWS services\n",
    "- supports PCI DSS Compliance (credit card framework)\n",
    "\n",
    "### Key terminology for IAM\n",
    "\n",
    "- Users: \n",
    "    - end users sch as people, employees of an organization etc\n",
    "- Groups:\n",
    "    - a collection of users. Each user in the group will inherit the permissions of the group\n",
    "- Policies:\n",
    "    policies are made up of documents, called policy documents\n",
    "- Policiy Documents:\n",
    "    - documents are in JSON and give permission as to what a User/Group/Role is able to do\n",
    "    \n",
    "- Roles\n",
    "    - you create roles and then assign them to AWS resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity Access Management - LAB\n",
    "\n",
    "\n",
    "### Activate multifactor security\n",
    "\n",
    "### Creating an individual IAM group\n",
    "\n",
    "- Create a new user with a unique name\n",
    "- assign user a group by selecting what privileges to add\n",
    "- optional to create tags\n",
    "- can send him an email\n",
    "- can download credentials \n",
    "\n",
    "### Creating a group policy \n",
    "\n",
    "every group starts with no policy and has to be added.\n",
    "\n",
    "All group policies are in JSON format for this case admin access\n",
    "\n",
    "Version: date version\n",
    "Statement\n",
    "    - Effect : Allow\n",
    "    - Action : * this means anything\n",
    "    - Resource: *\n",
    "    \n",
    "### Password policy\n",
    "\n",
    "Can have requirements for password like:\n",
    "- Minimum password length is {12} characters\n",
    "- Require at least one uppercase letter from Latin alphabet (A-Z)\n",
    "- Require at least one lowercase letter from Latin alphabet (a-Z)\n",
    "- Require at least one number\n",
    "- Require at least one non-alphanumeric character (!@#$%^&*()_+-=[]{}|')\n",
    "- Enable password expiration\n",
    "- Password expiration requires administrator reset\n",
    "- Allow users to change their own password\n",
    "- Prevent password reuse\n",
    "\n",
    "In the console you can alter passwords, like deleting or resending password to users.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roles \n",
    "\n",
    "Allow on AWS service to use another AWS service\n",
    "\n",
    "like allows a virtual machine to talk to S3\n",
    "\n",
    "### Exam tips\n",
    "\n",
    "What have we learnt so far?\n",
    "\n",
    "- IAM is universal, it does not apply to regions at this time\n",
    "- the 'root account' is the account created when first setup your AWS account. It has complete Admin access\n",
    "- New users have **NO permissions** when first created\n",
    "- New users are assigned Access Key ID and Secret Access Keys when first created\n",
    "    - Can have programatic access or console access.\n",
    "- These are not the same as a password. you cannot use the access key ID and secret access key to login into the console. you can use this to access AWS via the API's and command line, however\n",
    "- You only get to view these once. if you lose them, you have to regenerate them.\n",
    "- always setup mulltifactor authentication on your root account\n",
    "- you  can create and customise your own password rotation policies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a billing alarm - lab\n",
    "\n",
    "CloudWatch - way to monitor your cloud account\n",
    "\n",
    "- Go into cloud watch create billing\n",
    "- select  create alarm\n",
    "- select metric\n",
    "- select instance to observe\n",
    "- specify metric parameters or conditions\n",
    "- select notification trigger \n",
    "- give it a description \n",
    "- preview and commit\n",
    "\n",
    "in exam, how can you get automatic notifications if account goes over 1,000?\n",
    "\n",
    "You can create a billing alarm in cloud watch, have the billing alarm send a sns topic to email.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 101\n",
    "\n",
    "### What is S3?\n",
    "\n",
    "S3 provides developers and IT teams with secure, durable, highly scalable object storage. Amazon S3 is easy to use, with a simple web services interface to store and retrieve any amount of data from anywhere on the web.\n",
    "\n",
    "### So what is S3?\n",
    "- S3 is a safe place to store your files\n",
    "- it is object-based storage\n",
    "- the data is spread across multiple devices and facilities\n",
    "\n",
    "### The basics of s3 are:\n",
    "- S3 is object-base and allows you to upload files\n",
    "    - Key: or filename of the object\n",
    "    - Value: the data being stored\n",
    "    - Version Id: important for versioning\n",
    "    - Metadata: data about your data\n",
    "    - Subresources:\n",
    "        - access control lists\n",
    "        - torrent \n",
    "    \n",
    "- files can be from 0 Bytes to 5 TB\n",
    "- There is unlimited storage\n",
    "- files are stored in buckets (sort of like folders)\n",
    "- S3 is a universal namespace, that is names must be unique globally\n",
    "    - this is because it creates a web addressed based on its name\n",
    "    - given as https://bucketname.regionlocation.amazonaws.com/\n",
    "- when you upload a file to S3, you will receive a HTPP 2-- code f the upload was successful\n",
    "\n",
    "\n",
    "### How does data consistency work for S3?\n",
    "- Read after write consistency for PUTS of new objects\n",
    "    - if you upload a file to S3, you are able to read it immediately\n",
    "- Eventual consistency for overwrite PUTS and DELETES (can take time to propagate)\n",
    "    - if you change the file, you will have to wait for it to update. will get an eariler version if it is not ready\n",
    "\n",
    "In other words;\n",
    "- if you write a new file and read it immediately aftwards, you will be able to view that data\n",
    "- if you update AN EXISTING file or delete a file and read it immediately, you may get the older version, or you may not. Changes to objects can take time to propagate.\n",
    "\n",
    "S3 has the following guarantees from Amazon;\n",
    "- built for 99.99% availability for the S3 platform\n",
    "- Amazon guarantees 99.95 availability\n",
    "- amazon guarantees 99.99999999% durability for S3 information. thats 11 x9s\n",
    "\n",
    "S3 has the following features;\n",
    "- Tiered storage available\n",
    "- lifecycle management\n",
    "- versioning\n",
    "- encryption\n",
    "- MFA Delete\n",
    "- secure your data using access control lists and bucket policies\n",
    "\n",
    "### S3 storage classes:\n",
    "1. S3 Standard\n",
    "    - 99.99% availability\n",
    "    - 99.999999999% durability\n",
    "    - stored redundantly across multiple devices in multiple facilities, and is designed to sustain the loss of 2 facilities concurrently\n",
    "2. S3 - IA (infrequently Accessed)\n",
    "    - for data that is accessed less frequently, but requires rapid access when needed. Lower fee than S3, but you are charged a retrieval fee\n",
    "3. S3 One Zone -IA\n",
    "    - for where you want a lower-cost option for infrequently access data, but do not require the multiple availability zone data resilience\n",
    "4. S3 - Intelligent Tiering\n",
    "    - optimize costs by automatically moving data to the most cost-effective access tier without performance impact or operational overhead\n",
    "5. S3 Glacier\n",
    "    - a secure, durable and low-cost storage class for data archiving. you can reliably store any amount of data at costs that are competive with or cheaper than on-premises solutions. Retrieval times configurable from minutes to hours.\n",
    "6. S3 Glacier Deep Archive\n",
    "    - S3 lowest-cost storage class where a retrieval time of 12 hours is acceptable.\n",
    "    \n",
    "![Image of S3 tiers](https://raw.githubusercontent.com/JonathanWamsley/AWS-Certified-Solutions-Architect-Associate-2020/master/images/S3%20comparision.JPG)\n",
    "\n",
    "### S3- Charges\n",
    "\n",
    "- storage\n",
    "- requests\n",
    "- storage management pricing\n",
    "- data transfer pricing\n",
    "- transfer acceleration\n",
    "- cross region replication pricing\n",
    "\n",
    "### cross region replication\n",
    "\n",
    "When you upload a file to a bucket in one region and it is automatically replicated to other buckets in other regions\n",
    "\n",
    "### S3 Transfer Acceleration\n",
    "\n",
    "- Amazon S3 Transfer Acceleration enables fast, easy and secure transfers of files over long distances between your end user and S3 buckets.\n",
    "- tranfer acceleration takes advantage of Amazon CloudFront's globally distributed edge locations.\n",
    "    - As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path\n",
    "  \n",
    "take-away: speeds up time users get a file by accessing local edge locations to retreive data instead of at the source region, which can be much further\n",
    "\n",
    "### Exam Tips\n",
    "\n",
    "- Remember that S3 is Object-based: allows you to upload files\n",
    "- files can be 0 to 5 TB\n",
    "- there is unlimited storage\n",
    "- files are stored in Buckets\n",
    "- S3 is a universtal namespace, that is names must be unique globally\n",
    "- a file can be seen as a bucketname.s3.amazonaws.com in default region\n",
    "- or at other regions, bucketname.regionname.amazonaws.com\n",
    "\n",
    "- not suitable to install an operating system or databsase on, will want block based storage for that. S3 is object based storage\n",
    "\n",
    "- when you successfully upload a file you will get HTTP 200 status code\n",
    "- you can turn on MFA delete\n",
    "\n",
    "The key fundamentals of S3 are:\n",
    "- Key\n",
    "- Value\n",
    "- Version\n",
    "- Metadata\n",
    "- Subresources\n",
    "    - access control lists\n",
    "    - torrent\n",
    "    \n",
    "- read after write consistentcy for PUTS of new objects\n",
    "- eventual consistency for overwrite PUTS and DELETES\n",
    "\n",
    "Storage Classes:\n",
    "- S3 Standard\n",
    "- S3 - IA\n",
    "- S3 - One Zone - IA\n",
    "- S3 - intelligent tiering\n",
    "- S3 - Glacier\n",
    "- S3 - Glacier Deep Archeive\n",
    "\n",
    "Tip read S3 FAQs before taking exam and get hands dirty\n",
    "https://aws.amazon.com/s3/faqs/\n",
    "\n",
    "(will do notes on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an S3 Bucket-lab\n",
    "\n",
    "### Steps\n",
    "- go to S3 in the AWS console\n",
    "- create a bucket name(like a file) and it must be unique\n",
    "- select region  \n",
    "\n",
    "**Bucket features that can be turned on**\n",
    "- versioning\n",
    "- service access logging\n",
    "- tags of key/value pairs\n",
    "- object level logging\n",
    "- encryption\n",
    "- cloudwatch to monitor s3\n",
    "\n",
    "### more steps\n",
    "- AWS makes it default to no public access\n",
    "- a summary is shown before you create the bucket\n",
    "\n",
    "- can now go into bucket\n",
    "- upload a file\n",
    "- shows a success (200 status code)\n",
    "- can click on the file and see the overview, properties and permissions\n",
    "\n",
    "Right now the object is not public\n",
    "- go to bucket and edit public access settings \n",
    "- uncheck block all public access\n",
    "- go to file, and change to make public under actions\n",
    "\n",
    "### Overview\n",
    "Can change the bucket storage class in the properties change storage class\n",
    "\n",
    "### Properties\n",
    "To do changes to a bucket as a whole, go to properties tab and change on bucket level.\n",
    "\n",
    "### Permisions\n",
    "The access control list can set access level for each bucket or individual file\n",
    "bucket policies are applied to the whole bucket\n",
    "\n",
    "### Management\n",
    "\n",
    "looks at lifecycle and replication rules\n",
    "\n",
    "There are a lot more features is their S3 Master class\n",
    "\n",
    "### Exam tips\n",
    "\n",
    "Control access to buckets using either a bucket ACL or using Bucket Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 pricing tiers\n",
    "\n",
    "Popular exam question is which tier of S3 should you use for a given scenario?\n",
    "\n",
    "### What makes up the cost of S3?\n",
    "\n",
    "- storage\n",
    "- requests and data retrievals\n",
    "- data transfer\n",
    "- management and replication\n",
    "\n",
    "### What are the different Tiers?\n",
    "\n",
    "Cost decreases per GB , more GB less it cost\n",
    "1. standard\n",
    "2. IA\n",
    "3. one zone - IA\n",
    "4. inteliigent tiering\n",
    "5. glacier\n",
    "6. gllacier deep archive\n",
    "\n",
    "![s3 tier price](https://raw.githubusercontent.com/JonathanWamsley/AWS-Certified-Solutions-Architect-Associate-2020/master/images/S3%20price%20by%20tier.JPG)\n",
    "\n",
    "https://aws.amazon.com/s3/storage-classes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Security and Encryption\n",
    "\n",
    "By default, all newly created buckets are PRIVATE. You can setup access control to your buckets using;\n",
    "\n",
    "- Bucket Policies\n",
    "- Access Control Lists\n",
    "\n",
    "S3 buckets can be configured to create access logs which log all requests made to the S3 bucket. This can be sent to another bucket and even another bucket in another account.\n",
    "\n",
    "### Encryption in transit is archieved by\n",
    "\n",
    "- when you go to a website thats in https, the website is encrypted in transit, that is between your computer and the server.\n",
    "- Achieved by SSL/ TLS (Client Side)\n",
    "- Encryption at rest is (Server Side) encryption of data that is being stored.\n",
    "    - A word document encrypted means others can not read it if the file is taken\n",
    "\n",
    "AWS can only help with Server Side encryption in 3 ways\n",
    "1. S3 Management Keys - SSE-S3 (server side encryption s3)\n",
    "    - amazon manages keys for you(way to encrypt and decrypt file)\n",
    "2. AWS Key Management Service, Managed Keys - SSE KMS\n",
    "    - You and amazon manage keys together\n",
    "3. Server Side Encrpytion with customer provided keys - SSE-C\n",
    "    - Customer give amazon own keys\n",
    "- can also encrypt on the client side. Encrypt then use AWS to put into S3 bucket\n",
    "\n",
    "### Encrypting in AWS console\n",
    "\n",
    "- go into S3\n",
    "- click in bucket\n",
    "- click on file\n",
    "- look at properties, encryption\n",
    "- can change to services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Versioning - Lab\n",
    "\n",
    "Using versioning with S3:\n",
    "- Stores all versions of an object (including all writes and even if you delete an object)\n",
    "- great backup tool\n",
    "- once enabled, versioning cannot be disabled, only suspended\n",
    "- integrates with lifecycle rules\n",
    "- versioning MFA delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security\n",
    "\n",
    "Using the previous public bucket \n",
    "- go to versioning and enable\n",
    "- add another file\n",
    "- edit the file\n",
    "- access of the file is now denied.\n",
    "- make public again\n",
    "- can now see it again\n",
    "- can see versions in bucket name toggling show. shows last modified date, version ID. size and storage clasee\n",
    "\n",
    "To delete\n",
    "- go to actions delete(your folders is now empty) but you can show versions again\n",
    "- has a delete and shows version history\n",
    "- restore by deleting the delete marker\n",
    "- most recent version is now shown\n",
    "\n",
    "\n",
    "### exam tips\n",
    "\n",
    "-stores all versions of an object (including all writes and even if you delete an object)\n",
    "- great backup tool\n",
    "- once enabled, versioning cannot be disabled, only suspended\n",
    "- integrates with lifecycle rules\n",
    "- versioning's MFA delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifecycle management with S3 - LAB\n",
    "\n",
    "in S3 bucket\n",
    "- under management go to lifecycle rule\n",
    "- will automate transitioning your bucket to different tiers of storage\n",
    "- can expire objects as well\n",
    "\n",
    "Creating a lifecycle rule:\n",
    "- enter name of rule\n",
    "- storage class transition\n",
    "    - enable for current version\n",
    "    - enable for previous version\n",
    "    - establish a tier translation in X days\n",
    "- configure expiration\n",
    "    - current version\n",
    "    - prevous version\n",
    "    - clean up expires object delete markers and incomplete multipart uploads\n",
    "        - clean up expired object delete markers (specify days)\n",
    "        - clean up incomplete multipart uploads (specify days)\n",
    "- confirm in summary\n",
    "        \n",
    "### exam tips\n",
    "\n",
    "- automate moving your objects between the different storage tiers\n",
    "- can be used in conjunction with versioning\n",
    "- can be applied to current versions and previous versions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Organizations and Consolidating Billings\n",
    "\n",
    "# What is AWS Orgnaizations?\n",
    "\n",
    "\"AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create an centrally manage\"\n",
    "\n",
    "\n",
    "![aws organizations](https://docs.aws.amazon.com/organizations/latest/userguide/images/BasicOrganization.png)\n",
    "\n",
    "We have your root AWS account (master account)\n",
    "- best practices are to use this account for billing only\n",
    "- no resources deployed\n",
    "\n",
    "The OU are organizational units\n",
    "- developers, financial department, test, dev, etc\n",
    "- apply permissions by using policies\n",
    "    - applies a policy document that will then be inherited to the OU group\n",
    "    - Allow S3 Bucket, and EC2. Then all branches also have this policy\n",
    "\n",
    "Consolidating billing:\n",
    "- all your accounts are aggregated together\n",
    "- one bill per AWS account\n",
    "- very easy to track charges and allocate costs\n",
    "- volume pricing discount\n",
    "\n",
    "![](http://d1nqddva888cns.cloudfront.net/consolidated_billing_diagram.gif)\n",
    "\n",
    "\n",
    "Watching him use multiple accounts to set up AWS organizations\n",
    "\n",
    "- go into AWS organization in console\n",
    "- create organization (makes you the root account) and give an organization a name\n",
    "- can now invite account (can also create account in master acount)\n",
    "- enter acount email or account id\n",
    "    - will have to have the other account verify that email address\n",
    "    \n",
    "    \n",
    "- from the second account you recieve the invitation that can be accepted\n",
    "- second account has the option to leave the organization\n",
    "\n",
    "\n",
    "- from the master account can now see the 2nd account as the organization name\n",
    "- on the organize accounts tab, you can organize them and give them policies\n",
    "    - can create policies similar to IAM\n",
    "    \n",
    "### Exam tips\n",
    "\n",
    "- always enable MFA on root account\n",
    "- always use a strong and complex password on root account\n",
    "- paying account should be used for billing purposes only. Do not deploy resources into the paying account\n",
    "- enable/disable AWS services using service control policies (SCP) either on OU or on individual accounts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab- Sharing S3 Buckets Across Accounts\n",
    "\n",
    "### 3 different ways to share S3 buckets across accounts\n",
    "\n",
    "- using bucket policies and IAM (applies across the entire bucket), Programmatic access only\n",
    "- Using buckets ACLs and IAM (individual objects). Programmatic access only\n",
    "- cross-account IAM Roles. programatic and console access\n",
    "\n",
    "Roles: a way of temporarily granting access to an AWS resource, either from another AWS service such as EC2 or other AWS accounts\n",
    "\n",
    "starting in aws organizations root account:\n",
    "- go to IAM and select create role\n",
    "    - Select another AWS account\n",
    "    - put the account number of the other account you are giving permision to\n",
    "    - attach policies to the role (AmazonS3FullAccess) with a name\n",
    "    - click into role created\n",
    "        - there is a link that you give to users who can switch role in the console\n",
    "- in second account, go to IAM\n",
    "    - go to users add users (give AWS Management console access)\n",
    "    - (can give custom or autogenerated password)\n",
    "    - add user to group (admin access) and create user\n",
    "\n",
    "- in third account with that new user created with admin access sign in\n",
    "    - has a switch role under the name drop bar\n",
    "    - can use the role that was created in 1st account with the role url link\n",
    "    - shows a cross account access under name\n",
    "    - no permissions for billings (only S3 permission)\n",
    "    - can create a bucket and access previous buckets on account 1\n",
    "    \n",
    "### exam tips\n",
    "\n",
    "3 different ways to share S3 buckets accross acounts\n",
    "\n",
    "- using bucket policies and IAM (across buckets)\n",
    "- using buckets ACLs and IAM (individual objects)\n",
    "- cross-account IAM Roles. (programmatic and console access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross region replication-lab\n",
    "\n",
    "in AWS console\n",
    "- in buckets, go to management\n",
    "    - to to replication\n",
    "        - add rule (requires versioning to be enabled)\n",
    "            - can select entire bucket or files or (prefix/tags)\n",
    "            - can assign a destination bucket(in this or other account)\n",
    "            - create DNS bucket name and select region (can change ownership/storage class)\n",
    "            - select IAM role and role name\n",
    "- In buckets, you see the replicated bucket but the files are not there\n",
    "- as soon as you make changes to a bucket with cross region replication turn on it will update\n",
    "- upload new file in the original bucket\n",
    "- view new bucket at different region (turn public) and view\n",
    "\n",
    "Note if you delete an original bucket, it does not put a delete marker on the cross region buckets\n",
    "\n",
    "if you delete the latest version in the original bucket, then the cross region still shows the latest version that was deleted in the original bucket\n",
    "\n",
    "### Exam tips\n",
    "\n",
    "- versioning must be enabled on both the source and destination buckets\n",
    "- regions must be unique\n",
    "- files in an existing bucket are not replicated automatically\n",
    "- all subsequent updates files will be replicated automatically\n",
    "- delete markers are not replicated\n",
    "- deleting individual versions or delete markers will not be replicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Transfer Acceleration\n",
    "\n",
    "### what is S3 acceleration\n",
    "\n",
    "S3 transfer accesleration utilises the CloudFront Edge Network to accelerate your uploads to S3. Instead of uploading directly to your S3 bucket, you can use a distinct URL to upload directly to and edge location which will then transfer that file to S3. You will get a distinct URL to upload to:\n",
    "\n",
    "BucketName.s3-accelerate.amazonaws.com\n",
    "\n",
    "So users around the world can upload a file to an edge location that will then upload to our S3 bucket\n",
    "\n",
    "There is a tool to test transfer acceleration\n",
    "https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html\n",
    "\n",
    "compares how much faster it works on world wide regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is CloudFront?\n",
    "\n",
    "### common terms \n",
    "\n",
    "- CloudFront: A content delivery network (CDN) is a system of distributed servers (network) that delivers webpages and other web content to a user based on the geographic locations of the user, the origins of the webpage, and a content delivery server.\n",
    "\n",
    "- Edge Location: this is a location where content will be cached. THis is seperate to an AWS Region/AZ.\n",
    "\n",
    "- Origin: This is the origin of all the files that the CDN will distribute. This can be an S3 Bucket, an EC2 instance, an elastic load balancer or route53\n",
    "\n",
    "- Distribution: this is the name given the CDN which consists of collection of Edge Locations\n",
    "\n",
    "### simple example\n",
    "\n",
    "Without a CDN, a user from lets say Austrailia wants a service in the US would have to pull directly from the US server which can take time.\n",
    "\n",
    "With a CDN, a user from lets say Austrailia wants a service in the US would now connect to an Edge Location if there is a copy of the content on it. If there is no copy then it will download a copy to that location to be cached for (lets say 72 hours). Then if another user grabs that info he will grab the cached version and the time will increase.\n",
    "\n",
    "### Amazon CloudFront features\n",
    "\n",
    "Amazon CloudFront can be used to deliver your entire website, including dynamic, static, streaming, and ineteractive content using a global network of edge locations. Requests for your content are automatically routed to the nearest edge location, so content is delivered with the best possible performance. \n",
    "\n",
    "### Types of distributions\n",
    "\n",
    "- web distribution: typically used for websites\n",
    "- RTMP: used for media streaming\n",
    "\n",
    "### exam tips\n",
    "\n",
    "- Edge Location: this is the location where content will be cached. This is seperate to an AWS Region/AZ\n",
    "- Origin: this is the origin of all the files that the CDN will distribute. This can be either an S3 Bucket, an EC2 instance, and Elastic Load Balancer, or Route53\n",
    "- Distribtion: this is the name given the CDN which consists of a collection of Edge Locations\n",
    "- Web Distribution: typically used for websites\n",
    "- RTMP: used for media streaming\n",
    "- Edge locations are not just READ only, you can write to them too (put an object on them)\n",
    "- obects are cached for the life of the TTL(time to live)\n",
    "-  you can clear cached objects, but you will be charged(can invalidate cached objects at a cost $$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creatingg a CloudFront Distribution - LAB\n",
    "\n",
    "- In S3, will be using a bucket as the origin that will attached the CloudFront distribution to.\n",
    "\n",
    "- In AWS CloudFront distribution (is a global service) click create ditribtion\n",
    "    - can create web distribution (clicks on this one)\n",
    "    - can create RTMP distribtion\n",
    "    \n",
    "- click on origin domain name, which displays all origins\n",
    "    - select the S3 bucket used earlier \n",
    "    - leave everything else default and create\n",
    "- can take time to set up(hour?)\n",
    "- select domain name and copy it, (dtnj0cfnndcgw.cloudfront.net)\n",
    "- go to S3 services\n",
    "- type dtnj0cfnndcgw.cloudfront.net.BucketName into browser to deliver content using edge location\n",
    "- go to cloudfront distribution\n",
    "    - in invalidation\n",
    "    - can invalidate files in the bucket\n",
    "    - go and select then disable it  (takes time)\n",
    "    - go and delete it after its prcoessed\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowball\n",
    "\n",
    "### What is Snowball?\n",
    "\n",
    "- Snowball is a petabyte-scale data transport solution that uses secure appliances to tranfer large amounts of data into and out of AWS (a physical container)\n",
    "- Using Snowball Addresses common challenges with large-scale data transfers including:\n",
    "    - high network costs\n",
    "    - long transfer time\n",
    "    - security conerns\n",
    "- Transferring data with Snowball is:\n",
    "    - simple\n",
    "    - fast\n",
    "    - secure\n",
    "    - can be one-fifth the cost of high-speed internet\n",
    "    \n",
    "### Snowball Info\n",
    "\n",
    "- Comes in 50TB or 80 TB\n",
    "- Uses multiple layers of secuirty designed to protect your data including\n",
    "    - temper-resistent enclosures\n",
    "    - 256-bit encryption\n",
    "    - industry-standard Trusted Platform Module (TPM)\n",
    "        - designed to ensure both secuirty and full chan-of-custody of your data\n",
    "- Once data transfer job has been procced and verified, AWS performs a software erasure of the Snowball appliance\n",
    "\n",
    "\n",
    "### Snowball Edge\n",
    "\n",
    "- comes in 100 TB data\n",
    "- data transfer device with on-board storage and compute capabilities\n",
    "- you can use snowball edge to move large amounts of data into and out of AWS as temporary storage tier for:\n",
    "    - large local datasets\n",
    "    - support local workloads in remote or offline locations\n",
    "\n",
    "(Like having a mini AWS at your disposole)\n",
    "\n",
    "- snowball edge connects to your existing applications and infrastructure using standard storage interfaces, streamlining the data transfer process and minimizing setup and integration.\n",
    "- snowball edge can cluster together to form a local storage tier and process your data on-premises, helping ensure your applications continue to run even when they are not able to access the cloud\n",
    "\n",
    "### What is Snowmobile\n",
    "\n",
    "![lol](https://raw.githubusercontent.com/JonathanWamsley/AWS-Certified-Solutions-Architect-Associate-2020/master/images/AWS%20snowmobile.png)\n",
    "\n",
    "- AWS Snowmobile is an Exabyte-scale data transfer service used to move exremely large amounts of data to AWS.\n",
    "- you can transfer up to 100PB per snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck\n",
    "- snowmobile makes it easy to move massive volumes of data to the cloud including:\n",
    "    - video\n",
    "    - libraries\n",
    "    - image repositories\n",
    "    - complete data center migration\n",
    "- transferring data with snowmobile is secure, fast and cost effective.\n",
    "\n",
    "### When should I use Snowball?\n",
    "\n",
    "![comparison](https://raw.githubusercontent.com/JonathanWamsley/AWS-Certified-Solutions-Architect-Associate-2020/master/images/Snowball%20comparision.JPG)\n",
    "\n",
    "When internet connection is slow and data size is high\n",
    "\n",
    "\n",
    "### exam tips\n",
    "\n",
    "- snowball can:\n",
    "    - import to S3\n",
    "    - export from S3\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Gateway\n",
    "\n",
    "### What is storage gateway?\n",
    "\n",
    "- Storage Gateway: a service that connects an on-premises software applicance with cloudbased storage to provide seamless and secure integration between an organization's on-premises IT enviorment and AWS's storage infrastructure\n",
    "- The service enable you to securely store data to the AWS cloud for scalable and cost-effective storage\n",
    "\n",
    "\n",
    "-Storage gateway's software applicaiance is available for download as a virtual machine (VM) image that you install on a host in your datacenter\n",
    "- supports either your gateway and asscociated it with your aws account through the activation process, you can use the aws management console to create the storage gateway option that is right for you\n",
    "\n",
    "### The three different types of storage\n",
    "\n",
    "- file gateway(NFS & SMB)\n",
    "- volume gateway (iSCSI)\n",
    "    - store volumes\n",
    "    - cached volumes\n",
    "- tape gateway (VTL)\n",
    "\n",
    "### file gateway\n",
    "\n",
    "files are stored as objects in your S3 buckers, accessed through a network file system (NFS) mount point. Ownership, permissions, and timestamps are durably stored in S3 in the user-metadate of the object associated with the file. Once onjects are transferred to S3, they can be managed as native S3 objects, and bucket policies such as versioning, lifecylce management, and cross-region replication apply directly to objects stored in your bucker.\n",
    "\n",
    "2:30 \n",
    "\n",
    "STOPPED IN MIDDLE :(\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
